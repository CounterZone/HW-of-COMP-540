\documentclass{article}
\usepackage{savetrees}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{geometry}
\usepackage{amsthm}
\usepackage{amssymb}

\begin{document}
\title{Solution to Homework 2}
\author{Shoeb Mohammed and Zhuo Chen}
\maketitle

\newcommand{\QEDA}{\hfill\ensuremath{\blacksquare}}
\newcommand{\QEDB}{\hfill\ensuremath{\square}}

\section{Gradient and Hessian of $NLL(\theta)$ for logistic regression}
\subsection{}
Given
\begin{equation}
  \label{eq:1.1}
  g(z) = \frac{1}{1+e^{-z}}
\end{equation}

\begin{proof}
\begin{equation}
  \label{eq:1.2}
  \frac{\partial g(z)}{\partial z} = \frac{(1+e^{-z}).0 + e^{-z}}{(1+e^{-z})^2} = \frac{e^{-z}}{(1+e^{-z})^2} = g(z)(1-g(z))
\end{equation}
\end{proof}
\subsection{}

\newcommand{\HthetaXi}{h_\theta(x_i)}

For logistic regression, negative log likelihood is
\begin{equation}
  \label{eq:1.3}
  NLL(\theta)  = - \sum_{i=1}^{m} \left( y_i log(\HthetaXi) + (1-y_i)log(1-\HthetaXi) \right) \text{ where } \HthetaXi = g(\theta^Tx^i)
\end{equation}

\begin{proof}
Using equation~\ref{eq:1.2} and chain rule for differentiation we have
\begin{equation}
  \label{eq:1.4}
  \begin{split}
  NLL(\theta)  &= - \sum_{i=1}^{m} \left( \frac{y_i}{\HthetaXi}\HthetaXi(1-\HthetaXi)\frac{\partial \theta^T x^i}{\partial \theta} 
                                         - \frac{(1-y_i)}{(1-\HthetaXi)}\HthetaXi(1-\HthetaXi)\frac{\partial \theta^T x^i}{\partial \theta} \right) \\
               &= - \sum_{i=1}^{m} \left( y_i(1-\HthetaXi) - (1-y_i)\HthetaXi\right) x_i \text{ because } \frac{\partial}{\partial \theta} \theta^T x^i = x^i \\
               &=   \sum_{i=1}^{m} \left(\HthetaXi - y_i\right)
  \end{split}
\end{equation}
\end{proof}


\subsection{}
Given

\begin{equation}
  \label{eq:1.5}
  \begin{split}
  H = X^T S X  \text{ where } &S = diag(\mu_1 \hdots \mu_m) \\
                              &\mu_i = \HthetaXi(1-\HthetaXi) \text{ for } i = 1 \hdots m \\
                              &\text{and } 0 < \mu_i < 1 \text{ for } i = 1 \hdots m
  \end{split}
\end{equation}

\begin{proof}
For any vector $u \neq 0$ we have,
\begin{equation}
  \label{eq:1.6}
  \begin{split}
    u^T H u  &= u^T (X^T S X) u \\
             &= (Xu)^T S (Xu)  \\
             &= v^T S v \text{ where } v = [v_1 \hdots v_m]^T = Xu \neq 0 \text{ since $X$ is full rank} \\
             &= \sum_{i=1}^{m}v_i^2 \mu_i \\
             &> 0 \text{ since $\mu_i$  is positive and $v_i \neq 0$} 
  \end{split}
\end{equation}

Thus, $H$ is positive definite.
\end{proof}

\section{Regularizing logistic regression}
\begin{proof}
The maximal likehood and MAP estimates for $\theta$ are

\newcommand{\thetaMAP}{\theta_{MAP}}
\newcommand{\thetaMLE}{\theta_{MLE}}

\begin{equation}
  \label{eq:1.7}
  \begin{split}
  \thetaMLE &= argmax_{\theta}\prod_{i=1}^m P(y^{(i)} | x^{(i)} ; \theta) \\
  \thetaMAP &= argmax_{\theta}P(\theta)\prod_{i=1}^m P(y^{(i)} | x^{(i)} ; \theta) \text{ where } P(\theta) \sim N(0,\alpha^2I)
  \end{split}
\end{equation}

Equation~\ref{eq:1.7} can be rewritten using log likelihood $LL(\theta)$:
\begin{equation}
  \label{eq:1.8}
  \begin{split}
  \thetaMLE &= argmax_{\theta}LL(\theta) \text{ where } LL(\theta) = \sum_{i=1}^m log(P(y^{(i)} | x^{(i)} ; \theta)) \\
  \thetaMAP &= argmax_{\theta} log(P(\theta)) + LL(\theta) \\
            &= argmax_{\theta} K - \frac{d}{2\alpha^2}\theta^T\theta + LL(\theta) \text{ where $K$ is constant. This follows from $P(\theta) \sim N(0,\alpha^2I)$}\\
            &= argmax_{\theta}  LL(\theta) - \frac{d}{2\alpha^2}\Vert \theta \Vert_2^2
  \end{split}
\end{equation}


Now,
\begin{equation}
  \label{eq:1.9}
  \begin{split}
   LL(\thetaMAP) - \frac{d}{2\alpha^2}\Vert \thetaMAP \Vert_2^2 &\geq    LL(\thetaMLE) - \frac{d}{2\alpha^2}\Vert \thetaMLE \Vert_2^2 \text{\qquad from definition for $\thetaMAP$} \\
                                                                &\geq    LL(\thetaMAP) - \frac{d}{2\alpha^2}\Vert \thetaMLE \Vert_2^2 \text{\qquad from definition for $\thetaMLE$} \\
   \implies \frac{d}{2\alpha^2}\Vert \thetaMAP \Vert_2^2 &\leq \frac{d}{2\alpha^2}\Vert \thetaMLE \Vert_2^2 \\
   \implies \Vert \thetaMAP \Vert_2^2 &\leq \Vert \thetaMLE \Vert_2^2 \\
   \implies \Vert \thetaMAP \Vert_2 &\leq \Vert \thetaMLE \Vert_2 
  \end{split}
\end{equation}
\end{proof}


\end{document}
