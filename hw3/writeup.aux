\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}MAP and MLE parameter estimation}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Logistic regression and Gaussian Naive Bayes}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Softmax regression and OVA logistic regression}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Implementing the loss function for softmax regression (naive version)}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Implementing the gradient of loss function for softmax regression(naive version)}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Implementing the loss function for softmax regression (vectorized version)}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Implementing the gradient of loss function for softmax regression(vectorized version)}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Implementing mini-batch gradient descent}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Using a validation set to select regularization lambda and learning rate for gradient descent}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Training a softmax classifier with the best hyperparameters}{3}}
